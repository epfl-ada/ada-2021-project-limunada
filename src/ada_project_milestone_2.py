# -*- coding: utf-8 -*-
"""ADA_project_milestone_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZoB2vn5q68gqHH8v8KI3hiDKPTKnMIkI

# ToDo

* map speaker attribute entries from Qids to meaningful labels
  * save all dictionaries as JSONs

* Jana - loading Quotebank per year, discarding None speakers, saving as JSON

## Installing and importing dependencies, mounting to drive
"""

!pip install tld
!pip install pyarrow
!pip install Wikidata
!pip install aspect_based_sentiment_analysis

import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt 
import json
import os
import bz2
import itertools 

# Import NLP library
import aspect_based_sentiment_analysis as absa
nlp = absa.load()

from google.colab import drive
drive.mount('/content/drive')

from wikidata.client import Client
wiki_client = Client()

"""## Load Quotebank and discard 'None' speakers"""

# Iterate through the years of existing Quotebank files
for year in range(2015, 2021):

  path_to_file = f'/content/drive/MyDrive/Quotebank/quotes-{year}.json.bz2' 
  path_to_out = f'/content/drive/MyDrive/Quotebank_limunADA/quotes-no-nones-{year}.json.bz2'

  # If it already exists, skip it
  if os.path.isfile(path_to_out):
    print(f'\nFile for year {year} already exists. Moving on...')
    continue

  print(f'\nExtracting non-None quotations for year {year}')

  with bz2.open(path_to_file, 'rb') as s_file:
    with bz2.open(path_to_out, 'wb') as d_file:
      for instance in s_file:
        # loading a sample
        instance = json.loads(instance) 
        
        if instance['speaker'] == 'None':
          continue

        # writing in the new file
        d_file.write((json.dumps(instance)+'\n').encode('utf-8'))

"""## Speaker attributes parquet"""

parquet_path = '/content/drive/MyDrive/Project datasets/speaker_attributes.parquet'
speaker_attributes = pd.read_parquet(parquet_path)

speaker_attributes.head()

"""## Quotebank samples from 2019"""

quotes_19_sample = pd.read_json(
    '/content/drive/MyDrive/quotes-2019-nytimes.json.bz2', 
    compression='bz2', 
    lines=True
    )

quotes_19_sample.head()

"""## Separating Trump quotes from the sample"""

TRUMP_ALIASES = ['Donald Trump', 'President Donald Trump', 'President Trump']

trump_quotes = quotes_19_sample[
  quotes_19_sample.apply(lambda row: row['speaker'] in TRUMP_ALIASES, axis=1)
  ]

trump_quotes.head()

"""## Get quotes mentioning given keywords (Biden in this case)"""

def get_mentions(quotes_df, keyword):
  """ 
  Returns a DataFrame of quotations containing the keyword in them. 
  """
  keyword = keyword.lower()
  mentions_bool = quotes_df.apply(
      lambda row: keyword in row['quotation'].lower(), axis=1
      )
  
  return quotes_df[mentions_bool]

biden_mentions = get_mentions(trump_quotes, 'biden')
biden_mentions.head()

"""## Add sentiment analysis columns (towards a keyword) to DataFrame"""

# Dictionary for mapping sentiment to index, corresponding to 
# the output of the NLP model from ABSA
SENTIMENT_INDEXING = {'neutral': 0, 'negative': 1, 'positive': 2}

def add_sentiment_columns(df_original, keyword, nlp_model):
  """ 
  Function that computes Aspect Based Sentiment Analysis towards the 
  given keyword, for each quote in the DataFrame.
  Also, it adds 3 columns (neutral, negative, positive) to the DataFrame, 
  containing sentiment scores corresponding to each of the columns.  
  """
  df = df_original.copy()
  for sentiment_key in SENTIMENT_INDEXING:
    df[sentiment_key] = 0

  for ind, row in df.iterrows():
    text = row['quotation'].lower()
    completed_task = nlp_model(text, aspects=[keyword])
    sentiment_scores = completed_task.examples[0].scores 

    for sentiment_key, sentiment_ind in SENTIMENT_INDEXING.items():
      df.loc[ind, sentiment_key] = sentiment_scores[sentiment_ind]

  return df

biden_sentiment = add_sentiment_columns(biden_mentions, 'biden', nlp)
biden_sentiment.head()

biden_sentiment.sort_values('negative', ascending=False, inplace=True)

examples_number = 5
for cnt, (ind, row) in enumerate(biden_sentiment.iterrows()):
  if cnt == examples_number:
    break
    
  print(f"\n\nNEGATIVE SCORE: {row['negative']}")
  print(f"POSITIVE SCORE: {row['positive']}")
  print(f"NEUTRAL SCORE: {row['neutral']}")

  print(row['quotation'])

"""## Wikidata Python API

### Extracting a set of QIDs from a given column 
"""

def get_qid_set(df, column_name, num_of_rows=None):
  """ 
  Given a DataFrame and column name, returns a set of QIDs in it. 
  """
  if num_of_rows is None:
    num_of_rows = len(df[column_name])

  # Join QIDs of each row (list -> string) to avoid working with np.array
  joint_qids_list = df[column_name].head(num_of_rows).to_frame().apply(
    lambda row: '' if row[0] is None else ','.join(row[0]), axis=1
    ).unique()

  # Iterate through the joint QIDs, split them, and add them to a set
  qids_set = set()
  for curr_joint_qids in joint_qids_list:
    for qid in curr_joint_qids.split(','):
      if not qid == '':
        qids_set.add(qid)

  return qids_set



def map_qids_to_labels(qids, wiki_client):
  """
  Given a set or list of QIDs, return a dictionary of format: {QID: label}
  We get the labels for each QID using the Wikidata client.
  """
  qids_labels_dict = dict()
  for qid in qids:
    try:
      # Multilingual to basic string
      qids_labels_dict[qid] = str(wiki_client.get(qid, load=True).label)
    except Exception:
      # In case the QID doesn't exist on Wikidata
      print(f'Problem with {qid}. Skipping...')

  return qids_labels_dict

"""### Get **genders** QID-label mapping"""

print('Getting QIDs set')
gender_qids = get_qid_set(speaker_attributes, 'gender')

print('Getting labels from Wikidata')
gender_label_dict = map_qids_to_labels(gender_qids, wiki_client)

dict(itertools.islice(gender_label_dict.items(), 10))

OVERWRITE_EXISTING = False

# Save the mapping
genders_save_path = '/content/drive/MyDrive/Quotebank_limunADA/genders_qids_labels.json'

if OVERWRITE_EXISTING or not os.path.isfile(genders_save_path):
  with open(, 'w') as f:
      json.dump(gender_label_dict, f)

"""### Get **occupations** QID-label mapping"""

print('Getting QIDs set')
occupation_qids = get_qid_set(speaker_attributes, 'occupation')

print('Getting labels from Wikidata')
occupation_label_dict = map_qids_to_labels(occupation_qids, wiki_client)

dict(itertools.islice(occupation_label_dict.items(), 10))

OVERWRITE_EXISTING = False

# Save the mapping
occupations_save_path = '/content/drive/MyDrive/Quotebank_limunADA/occupations_qids_labels.json'

if OVERWRITE_EXISTING or not os.path.isfile(occupations_save_path):
  with open(occupations_save_path, 'w') as f:
      json.dump(occupation_label_dict, f)

"""### Get **religions** QID-label mapping"""

print('Getting QIDs set')
religion_qids = get_qid_set(speaker_attributes, 'religion')

print('Getting labels from Wikidata')
religion_label_dict = map_qids_to_labels(religion_qids, wiki_client)

dict(itertools.islice(religion_label_dict.items(), 10))

OVERWRITE_EXISTING = False

# Save the mapping
religions_save_path = '/content/drive/MyDrive/Quotebank_limunADA/religions_qids_labels.json'

if OVERWRITE_EXISTING or not os.path.isfile(religions_save_path):
  with open(religions_save_path, 'w') as f:
      json.dump(religion_label_dict, f)

"""### Get **nationality** QID-label mapping"""

print('Getting QIDs set')
nationality_qids = get_qid_set(speaker_attributes, 'nationality')

print('Getting labels from Wikidata')
nationality_label_dict = map_qids_to_labels(nationality_qids, wiki_client)

dict(itertools.islice(nationality_label_dict.items(), 10))

OVERWRITE_EXISTING = False

# Save the mapping
nationality_save_path = '/content/drive/MyDrive/Quotebank_limunADA/nationalities_qids_labels.json'

if OVERWRITE_EXISTING or not os.path.isfile(nationality_save_path):
  with open(nationality_save_path, 'w') as f:
      json.dump(nationality_label_dict, f)

"""# Constructing a graph of individuals"""

nodes_qids = {
    'Q22686': 'Donald Trump', 
    'Q6294': 'Hillary Clinton', 
    'Q6279': 'Joe Biden',
    'Q76': 'Barack Obama'
    }
quotes_per_node = {k: [] for k in nodes_qids}
nodes_qids_set = set(nodes_qids.keys())

a = set([1])
a.pop()

# Iterate through the years of existing Quotebank files

year = 2016
path_to_file = f'/content/drive/MyDrive/Quotebank_limunADA/quotes-no-nones-{year}.json.bz2' 

print(f'\nExtracting quotes per node for year {year}')

with bz2.open(path_to_file, 'rb') as s_file:
  instance_cnt = 0
  for instance in s_file:
    # loading a sample
    instance = json.loads(instance) 
    
    qids_intersect = nodes_qids_set.intersection(set(instance['qids']))
    if len(qids_intersect) > 0:
      curr_qid = qids_intersect.pop()
      quotes_per_node[curr_qid].append(instance['quotation'])

    instance_cnt += 1
    if instance_cnt % 100000 == 0:
      print(f'Instance {instance_cnt}')

[len(v) for k, v in quotes_per_node.items()]

qids_list = list(nodes_qids.keys())
edges_qids = []
for pair in itertools.product(qids_list, qids_list):
  if pair[0] != pair[1]:
    edges_qids.append(pair)

print(edges_qids)

def get_mentions_from_list(quotes_list, look_for):
  # List of tuples of format (target_word, quote)
  mentions = []
  for quote in quotes_list:
    for target_word in look_for:
      if target_word.lower() in quote.lower():
        mentions.append((quote, target_word))
        break

  return mentions

SENTIMENT_INDEXING = {'neutral': 0, 'negative': 1, 'positive': 2}
SENTIMENT_WEIGHTS = {'neutral': 0, 'negative': -1, 'positive': 1}

def get_sentiment_score_nltk(text, nlp_model):
  scores = sia.polarity_scores(text)
  # if scores['neu'] > scores['neg'] and scores['neu'] > scores['pos']:
    # score = 0
  # else:
  score = scores['pos'] - scores['neg']
  return score
  # return scores['compound']

# edge_features = {}
for (speaker_qid, mention_qid) in edges_qids:
  print(f'Computing features for edge {speaker_qid} - {mention_qid}')
  
  mention_name = nodes_qids[mention_qid]
  look_for = mention_name.split()
  
  curr_mentions = get_mentions_from_list(
      quotes_per_node[speaker_qid], look_for
      )

  if len(curr_mentions) == 0:
    print('No mentions for this edge. Skipping... \n')
    continue 

  print(f'Number of mentions {len(curr_mentions)}\n')

  all_scores = []
  for mention_iter, mention in enumerate(curr_mentions):
    if mention_iter % 20 == 0:
      print(f'Mention number {mention_iter}')

    # curr_score = get_sentiment_score(*mention, nlp)
    curr_score = get_sentiment_score_nltk(mention[0], sia)
    all_scores.append(curr_score)

  all_scores_np = np.array(all_scores)
  pos_neg_ratio = np.sum(all_scores_np > 0) / np.sum(all_scores_np < 0)

  mean_score = np.mean(all_scores)
  num_mentions = len(curr_mentions)

  edge_features[(speaker_qid, mention_qid)] = {
      'mean_sentiment': mean_score,
      'num_mentions': num_mentions,
      'pos_neg_ratio': pos_neg_ratio
      }

for key, val in edge_features.items():
  print(nodes_qids[key[0]], nodes_qids[key[1]])
  print(val, '\n')

nodes_qids

get_sentiment_score('i am the person who would hate gojko', 'hilary', nlp)

curr_mentions

import nltk 
nltk.download('vader_lexicon')

from nltk.sentiment import SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

sia.polarity_scores('I challenge hunter Biden to man up and debate me.')



