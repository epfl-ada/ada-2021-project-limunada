# -*- coding: utf-8 -*-
"""ADA_project_milestone_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IUwcrAVdJq0uLTWx4XvvPXSlSj2mvhio

# ToDo

* map speaker attribute entries from Qids to meaningful labels
  * save all dictionaries as JSONs

* Jana - loading Quotebank per year, discarding None speakers, saving as JSON

## Installing and importing dependencies, mounting to drive
"""

!pip install tld
!pip install pyarrow
!pip install Wikidata
!pip install aspect_based_sentiment_analysis

import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt 
import json
import os
import bz2
import itertools 

# Import NLP library
import aspect_based_sentiment_analysis as absa
nlp = absa.load()

from google.colab import drive
drive.mount('/content/drive')

from wikidata.client import Client
wiki_client = Client()

"""## Load Quotebank and discard 'None' speakers"""

# Iterate through the years of existing Quotebank files
for year in range(2015, 2021):

  path_to_file = f'/content/drive/MyDrive/Quotebank/quotes-{year}.json.bz2' 
  path_to_out = f'/content/drive/MyDrive/Quotebank_limunADA/quotes-no-nones-{year}.json.bz2'

  # If it already exists, skip it
  if os.path.isfile(path_to_out):
    print(f'\nFile for year {year} already exists. Moving on...')
    continue

  print(f'\nExtracting non-None quotations for year {year}')

  with bz2.open(path_to_file, 'rb') as s_file:
    with bz2.open(path_to_out, 'wb') as d_file:
      for instance in s_file:
        # loading a sample
        instance = json.loads(instance) 
        
        if instance['speaker'] == 'None':
          continue

        # writing in the new file
        d_file.write((json.dumps(instance)+'\n').encode('utf-8'))

"""## Speaker attributes parquet"""

parquet_path = '/content/drive/MyDrive/Project datasets/speaker_attributes.parquet'
speaker_attributes = pd.read_parquet(parquet_path)

speaker_attributes.head()

"""## Quotebank samples from 2019"""

quotes_19_sample = pd.read_json(
    '/content/drive/MyDrive/quotes-2019-nytimes.json.bz2', 
    compression='bz2', 
    lines=True
    )

quotes_19_sample.head()

"""## Separating Trump quotes from the sample"""

TRUMP_ALIASES = ['Donald Trump', 'President Donald Trump', 'President Trump']

trump_quotes = quotes_19_sample[
  quotes_19_sample.apply(lambda row: row['speaker'] in TRUMP_ALIASES, axis=1)
  ]

trump_quotes.head()

"""## Get quotes mentioning given keywords (Biden in this case)"""

def get_mentions(quotes_df, keyword):
  """ 
  Returns a DataFrame of quotations containing the keyword in them. 
  """
  keyword = keyword.lower()
  mentions_bool = quotes_df.apply(
      lambda row: keyword in row['quotation'].lower(), axis=1
      )
  
  return quotes_df[mentions_bool]

biden_mentions = get_mentions(trump_quotes, 'biden')
biden_mentions.head()

"""## Add sentiment analysis columns (towards a keyword) to DataFrame"""

# Dictionary for mapping sentiment to index, corresponding to 
# the output of the NLP model from ABSA
SENTIMENT_INDEXING = {'neutral': 0, 'negative': 1, 'positive': 2}

def add_sentiment_columns(df_original, keyword, nlp_model):
  """ 
  Function that computes Aspect Based Sentiment Analysis towards the 
  given keyword, for each quote in the DataFrame.
  Also, it adds 3 columns (neutral, negative, positive) to the DataFrame, 
  containing sentiment scores corresponding to each of the columns.  
  """
  df = df_original.copy()
  for sentiment_key in SENTIMENT_INDEXING:
    df[sentiment_key] = 0

  for ind, row in df.iterrows():
    text = row['quotation'].lower()
    completed_task = nlp_model(text, aspects=[keyword])
    sentiment_scores = completed_task.examples[0].scores 

    for sentiment_key, sentiment_ind in SENTIMENT_INDEXING.items():
      df.loc[ind, sentiment_key] = sentiment_scores[sentiment_ind]

  return df

biden_sentiment = add_sentiment_columns(biden_mentions, 'biden', nlp)
biden_sentiment.head()

biden_sentiment.sort_values('negative', ascending=False, inplace=True)

examples_number = 5
for cnt, (ind, row) in enumerate(biden_sentiment.iterrows()):
  if cnt == examples_number:
    break
    
  print(f"\n\nNEGATIVE SCORE: {row['negative']}")
  print(f"POSITIVE SCORE: {row['positive']}")
  print(f"NEUTRAL SCORE: {row['neutral']}")

  print(row['quotation'])

"""## Wikidata Python API

### Extracting a set of QIDs from a given column 
"""

def get_qid_set(df, column_name, num_of_rows=None):
  """ 
  Given a DataFrame and column name, returns a set of QIDs in it. 
  """
  if num_of_rows is None:
    num_of_rows = len(df[column_name])

  # Join QIDs of each row (list -> string) to avoid working with np.array
  joint_qids_list = df[column_name].head(num_of_rows).to_frame().apply(
    lambda row: '' if row[0] is None else ','.join(row[0]), axis=1
    ).unique()

  # Iterate through the joint QIDs, split them, and add them to a set
  qids_set = set()
  for curr_joint_qids in joint_qids_list:
    for qid in curr_joint_qids.split(','):
      if not qid == '':
        qids_set.add(qid)

  return qids_set



def map_qids_to_labels(qids, wiki_client):
  """
  Given a set or list of QIDs, return a dictionary of format: {QID: label}
  We get the labels for each QID using the Wikidata client.
  """
  qids_labels_dict = dict()
  for qid in qids:
    try:
      # Multilingual to basic string
      qids_labels_dict[qid] = str(wiki_client.get(qid, load=True).label)
    except Exception:
      # In case the QID doesn't exist on Wikidata
      print(f'Problem with {qid}. Skipping...')

  return qids_labels_dict

"""### Get **genders** QID-label mapping"""

print('Getting QIDs set')
gender_qids = get_qid_set(speaker_attributes, 'gender')

print('Getting labels from Wikidata')
gender_label_dict = map_qids_to_labels(gender_qids, wiki_client)

dict(itertools.islice(gender_label_dict.items(), 10))

OVERWRITE_EXISTING = False

# Save the mapping
genders_save_path = '/content/drive/MyDrive/Quotebank_limunADA/genders_qids_labels.json'

if OVERWRITE_EXISTING or not os.path.isfile(genders_save_path):
  with open(, 'w') as f:
      json.dump(gender_label_dict, f)

"""### Get **occupations** QID-label mapping"""

print('Getting QIDs set')
occupation_qids = get_qid_set(speaker_attributes, 'occupation')

print('Getting labels from Wikidata')
occupation_label_dict = map_qids_to_labels(occupation_qids, wiki_client)

dict(itertools.islice(occupation_label_dict.items(), 10))

OVERWRITE_EXISTING = False

# Save the mapping
occupations_save_path = '/content/drive/MyDrive/Quotebank_limunADA/occupations_qids_labels.json'

if OVERWRITE_EXISTING or not os.path.isfile(occupations_save_path):
  with open(occupations_save_path, 'w') as f:
      json.dump(occupation_label_dict, f)

"""### Get **religions** QID-label mapping"""

print('Getting QIDs set')
religion_qids = get_qid_set(speaker_attributes, 'religion')

print('Getting labels from Wikidata')
religion_label_dict = map_qids_to_labels(religion_qids, wiki_client)

dict(itertools.islice(religion_label_dict.items(), 10))

OVERWRITE_EXISTING = False

# Save the mapping
religions_save_path = '/content/drive/MyDrive/Quotebank_limunADA/religions_qids_labels.json'

if OVERWRITE_EXISTING or not os.path.isfile(religions_save_path):
  with open(religions_save_path, 'w') as f:
      json.dump(religion_label_dict, f)

"""### Get **nationality** QID-label mapping"""

print('Getting QIDs set')
nationality_qids = get_qid_set(speaker_attributes, 'nationality')

print('Getting labels from Wikidata')
nationality_label_dict = map_qids_to_labels(nationality_qids, wiki_client)

dict(itertools.islice(nationality_label_dict.items(), 10))

OVERWRITE_EXISTING = False

# Save the mapping
nationality_save_path = '/content/drive/MyDrive/Quotebank_limunADA/nationalities_qids_labels.json'

if OVERWRITE_EXISTING or not os.path.isfile(nationality_save_path):
  with open(nationality_save_path, 'w') as f:
      json.dump(nationality_label_dict, f)