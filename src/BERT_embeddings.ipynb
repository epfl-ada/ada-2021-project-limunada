{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "94b7470543774b6e946c3a2cc060be15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1e08a06fc61f43bfbf06941d92a6dd91",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6b1435bb2ae5496ebc65356eb03eccc2",
              "IPY_MODEL_fc9cc6db824045a681c541894eae87c3",
              "IPY_MODEL_11f67c39683a40b89d0e786f8ffab5b4"
            ]
          }
        },
        "1e08a06fc61f43bfbf06941d92a6dd91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b1435bb2ae5496ebc65356eb03eccc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_663ecfcba3844b13b3ee4d69d23f940d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Batches: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_18d145cf20bd4189b30a0d1bf10885f2"
          }
        },
        "fc9cc6db824045a681c541894eae87c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0ec9ad2941e3488c86dc88eb67f8c1be",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cdfc7651f41f431f9c414abdf4e6e751"
          }
        },
        "11f67c39683a40b89d0e786f8ffab5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ac7a4b65cd564022aae38d29848b94d1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9/9 [00:00&lt;00:00, 22.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e7e8ab938de54816928296ebca5991a6"
          }
        },
        "663ecfcba3844b13b3ee4d69d23f940d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "18d145cf20bd4189b30a0d1bf10885f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ec9ad2941e3488c86dc88eb67f8c1be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cdfc7651f41f431f9c414abdf4e6e751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac7a4b65cd564022aae38d29848b94d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e7e8ab938de54816928296ebca5991a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1shpoO0al7a4",
        "outputId": "c414c38c-b59d-42a8-e7f9-9e54759678aa"
      },
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install aspect_based_sentiment_analysis"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.96)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.10.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.8.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: aspect_based_sentiment_analysis in /usr/local/lib/python3.7/dist-packages (2.0.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from aspect_based_sentiment_analysis) (2.2.4)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (from aspect_based_sentiment_analysis) (2.10.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from aspect_based_sentiment_analysis) (1.18.1)\n",
            "Requirement already satisfied: testfixtures in /usr/local/lib/python3.7/dist-packages (from aspect_based_sentiment_analysis) (6.18.3)\n",
            "Requirement already satisfied: transformers==4.8 in /usr/local/lib/python3.7/dist-packages (from aspect_based_sentiment_analysis) (4.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from aspect_based_sentiment_analysis) (1.0.1)\n",
            "Requirement already satisfied: tensorflow==2.5 in /usr/local/lib/python3.7/dist-packages (from aspect_based_sentiment_analysis) (2.5.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from aspect_based_sentiment_analysis) (3.6.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from aspect_based_sentiment_analysis) (5.5.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (1.15.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (0.37.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (3.17.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (2.5.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (3.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (2.7.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (1.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (0.12.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (1.19.5)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (1.12)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (3.3.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5->aspect_based_sentiment_analysis) (1.34.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8->aspect_based_sentiment_analysis) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.8->aspect_based_sentiment_analysis) (4.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.8->aspect_based_sentiment_analysis) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.8->aspect_based_sentiment_analysis) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8->aspect_based_sentiment_analysis) (4.62.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8->aspect_based_sentiment_analysis) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.8->aspect_based_sentiment_analysis) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.8->aspect_based_sentiment_analysis) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.8->aspect_based_sentiment_analysis) (0.0.46)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8->aspect_based_sentiment_analysis) (0.0.12)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5->aspect_based_sentiment_analysis) (1.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.8->aspect_based_sentiment_analysis) (3.0.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5->aspect_based_sentiment_analysis) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5->aspect_based_sentiment_analysis) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5->aspect_based_sentiment_analysis) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5->aspect_based_sentiment_analysis) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5->aspect_based_sentiment_analysis) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5->aspect_based_sentiment_analysis) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5->aspect_based_sentiment_analysis) (1.8.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5->aspect_based_sentiment_analysis) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5->aspect_based_sentiment_analysis) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5->aspect_based_sentiment_analysis) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5->aspect_based_sentiment_analysis) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.8->aspect_based_sentiment_analysis) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5->aspect_based_sentiment_analysis) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8->aspect_based_sentiment_analysis) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8->aspect_based_sentiment_analysis) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8->aspect_based_sentiment_analysis) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8->aspect_based_sentiment_analysis) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5->aspect_based_sentiment_analysis) (3.1.1)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->aspect_based_sentiment_analysis) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->aspect_based_sentiment_analysis) (1.0.3)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->aspect_based_sentiment_analysis) (1.26.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->aspect_based_sentiment_analysis) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->aspect_based_sentiment_analysis) (2018.9)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->aspect_based_sentiment_analysis) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->aspect_based_sentiment_analysis) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->aspect_based_sentiment_analysis) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->aspect_based_sentiment_analysis) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->aspect_based_sentiment_analysis) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->aspect_based_sentiment_analysis) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->aspect_based_sentiment_analysis) (1.0.18)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->aspect_based_sentiment_analysis) (0.2.5)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna->aspect_based_sentiment_analysis) (1.4.27)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna->aspect_based_sentiment_analysis) (3.10.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna->aspect_based_sentiment_analysis) (6.6.0)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna->aspect_based_sentiment_analysis) (0.8.2)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna->aspect_based_sentiment_analysis) (1.4.1)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna->aspect_based_sentiment_analysis) (1.7.5)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna->aspect_based_sentiment_analysis) (1.1.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna->aspect_based_sentiment_analysis) (1.1.6)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna->aspect_based_sentiment_analysis) (5.4.0)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->aspect_based_sentiment_analysis) (0.4.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->aspect_based_sentiment_analysis) (2.3.3)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->aspect_based_sentiment_analysis) (3.5.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->aspect_based_sentiment_analysis) (2.4.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->aspect_based_sentiment_analysis) (5.8.0)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->aspect_based_sentiment_analysis) (1.8.2)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->aspect_based_sentiment_analysis) (21.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna->aspect_based_sentiment_analysis) (2.0.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->aspect_based_sentiment_analysis) (0.7.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->aspect_based_sentiment_analysis) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->aspect_based_sentiment_analysis) (8.12.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->aspect_based_sentiment_analysis) (1.11.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->aspect_based_sentiment_analysis) (0.7.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8->aspect_based_sentiment_analysis) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8->aspect_based_sentiment_analysis) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->aspect_based_sentiment_analysis) (3.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->aspect_based_sentiment_analysis) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->aspect_based_sentiment_analysis) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->aspect_based_sentiment_analysis) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->aspect_based_sentiment_analysis) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->aspect_based_sentiment_analysis) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->aspect_based_sentiment_analysis) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->aspect_based_sentiment_analysis) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->aspect_based_sentiment_analysis) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->aspect_based_sentiment_analysis) (0.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PROJECT_ROOT = '/content/drive/MyDrive'\n",
        "\n",
        "import os \n",
        "os.chdir(PROJECT_ROOT)\n",
        "DATA_PATH = os.path.join(PROJECT_ROOT, 'Quotebank_limunADA')\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "import spacy\n",
        "spacy.load('en')\n",
        "from spacy.lang.en import English\n",
        "parser = English()\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "import aspect_based_sentiment_analysis as absa\n",
        "nlp = absa.load()\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "import scipy\n",
        "import pickle\n",
        "import bz2\n",
        "import json\n",
        "from operator import itemgetter \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmIQW9BPFgzk",
        "outputId": "d745d6d6-7fb8-40ca-95e6-e02cccdbec4e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at absa/classifier-rest-0.2 were not used when initializing BertABSClassifier: ['dropout_379']\n",
            "- This IS expected if you are initializing BertABSClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertABSClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of BertABSClassifier were not initialized from the model checkpoint at absa/classifier-rest-0.2 and are newly initialized: ['dropout_75']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    lda_tokens = []\n",
        "    tokens = parser(text)\n",
        "\n",
        "    for token in tokens:\n",
        "        if token.orth_.isspace():\n",
        "            continue\n",
        "        elif token.like_url:\n",
        "            lda_tokens.append('URL')\n",
        "        elif token.orth_.startswith('@'):\n",
        "            lda_tokens.append('SCREEN_NAME')\n",
        "        else:\n",
        "            lda_tokens.append(token.lower_)\n",
        "\n",
        "    return lda_tokens\n",
        "\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "        return lemma\n",
        "    \n",
        "\n",
        "def get_lemma2(word):\n",
        "    return WordNetLemmatizer().lemmatize(word)\n",
        "\n",
        "\n",
        "def prepare_text_for_lda(text):\n",
        "    tokens = tokenize(text)\n",
        "    tokens = [token for token in tokens if len(token) > 4]\n",
        "    tokens = [token for token in tokens if token not in en_stop]\n",
        "    tokens = [get_lemma(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def get_tokens_per_quote(path_to_file, print_step=5e4, num_instances=None):\n",
        "\n",
        "    tokens_per_quote = {}\n",
        "    # Iterate through the quotes\n",
        "    with bz2.open(path_to_file, 'rb') as s_file:\n",
        "        for i, instance in enumerate(s_file):\n",
        "            if i % int(print_step) == 0:\n",
        "                print(f'Instance {i}')\n",
        "\n",
        "            if num_instances is not None:\n",
        "                if i == num_instances:\n",
        "                    break \n",
        "\n",
        "            # loading a sample and checking the speaker\n",
        "            instance = json.loads(instance) \n",
        "            tokens = prepare_text_for_lda(instance['quotation'])\n",
        "\n",
        "            tokens_per_quote[instance['quoteID']] = tokens \n",
        "    \n",
        "    return tokens_per_quote\n",
        "\n",
        "\n",
        "def get_instances_at_indexes(path_to_file, indexes=None, print_step=5e4):\n",
        "    instances = []\n",
        "    # Iterate through the quotes\n",
        "    with bz2.open(path_to_file, 'rb') as s_file:\n",
        "        for i, instance in enumerate(s_file):\n",
        "            if i % int(print_step) == 0:\n",
        "                print(f'Instance {i}')\n",
        "                \n",
        "            if indexes is None or i in indexes:\n",
        "                instance = json.loads(instance) \n",
        "                instances.append(instance['quotation'])\n",
        "            else:                \n",
        "                continue \n",
        "\n",
        "    return instances\n",
        "\n"
      ],
      "metadata": {
        "id": "cIyZpZNnFuKE"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "YEAR = '2019'\n",
        "\n",
        "LOAD_TOKENS_PER_QUOTE = True\n",
        "\n",
        "path_to_democrates = os.path.join(\n",
        "    DATA_PATH, f'quotes-democrates-{YEAR}.json.bz2'\n",
        "    )\n",
        "path_to_republicans = os.path.join(\n",
        "    DATA_PATH, f'quotes-republicans-{YEAR}.json.bz2'\n",
        "    )\n",
        "\n",
        "os.makedirs(os.path.join(DATA_PATH, 'SBERT'), exist_ok=True) \n",
        "\n",
        "path_to_democrates_tokens = os.path.join(\n",
        "    DATA_PATH, 'SBERT', f'tokens_per_quote_strings_democrates_{YEAR}.pkl'\n",
        "    )\n",
        "path_to_republicans_tokens = os.path.join(\n",
        "    DATA_PATH, 'SBERT', f'tokens_per_quote_strings_republicans_{YEAR}.pkl'\n",
        "    )\n",
        "\n",
        "if not LOAD_TOKENS_PER_QUOTE:\n",
        "    democrates_tokens_per_quote = get_tokens_per_quote(\n",
        "        path_to_democrates, num_instances=None\n",
        "        )\n",
        "    republicans_tokens_per_quote = get_tokens_per_quote(\n",
        "        path_to_republicans, num_instances=None\n",
        "        )\n",
        "    \n",
        "    democrates_tokens_per_quote_strings = \\\n",
        "        [' '.join(quote) for k, quote in democrates_tokens_per_quote.items()]\n",
        "    republicans_tokens_per_quote_strings = \\\n",
        "        [' '.join(quote) for k, quote in republicans_tokens_per_quote.items()]\n",
        "\n",
        "    pickle.dump(\n",
        "        democrates_tokens_per_quote_strings, \n",
        "        open(path_to_democrates_tokens, 'wb')\n",
        "        )\n",
        "    pickle.dump(\n",
        "        republicans_tokens_per_quote_strings, \n",
        "        open(path_to_republicans_tokens, 'wb')\n",
        "        )    \n",
        "\n",
        "else:\n",
        "    print('Loading democrates_tokens_per_quote_strings...')\n",
        "    democrates_tokens_per_quote_strings = pickle.load(\n",
        "        open(path_to_democrates_tokens, 'rb')\n",
        "        )\n",
        "        \n",
        "    print('Loading republicans_tokens_per_quote_strings...')\n",
        "    republicans_tokens_per_quote_strings = pickle.load(\n",
        "        open(path_to_republicans_tokens, 'rb')\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ31U9WSFuHt",
        "outputId": "b5079e5e-fea0-45f6-c8c9-a06a68c3e67a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading democrates_tokens_per_quote_strings...\n",
            "Loading republicans_tokens_per_quote_strings...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_QUOTES = True\n",
        "\n",
        "path_to_democrates_quotes_list = os.path.join(\n",
        "    DATA_PATH, 'SBERT', f'quotes_list_democrates_{YEAR}.pkl'\n",
        "    )\n",
        "path_to_republicans_quotes_list = os.path.join(\n",
        "    DATA_PATH, 'SBERT', f'quotes_list_republicans_{YEAR}.pkl'\n",
        "    )\n",
        "\n",
        "if not LOAD_QUOTES:\n",
        "    democrates_quotes = get_instances_at_indexes(path_to_democrates)\n",
        "    republicans_quotes = get_instances_at_indexes(path_to_republicans)\n",
        "\n",
        "    pickle.dump(\n",
        "        democrates_quotes, \n",
        "        open(path_to_democrates_quotes_list, 'wb')\n",
        "        ) \n",
        "    pickle.dump(\n",
        "        republicans_quotes, \n",
        "        open(path_to_republicans_quotes_list, 'wb')\n",
        "        ) \n",
        "    \n",
        "else:\n",
        "    print('Loading democrates_quotes...')\n",
        "    democrates_quotes = pickle.load(\n",
        "        open(path_to_democrates_quotes_list, 'rb')\n",
        "        )\n",
        "        \n",
        "    print('Loading republicans_quotes...')\n",
        "    republicans_quotes = pickle.load(\n",
        "        open(path_to_republicans_quotes_list, 'rb')\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPGJde3Ui380",
        "outputId": "e1d24956-1cdb-4762-8f85-b9c9d9dc1346"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading democrates_quotes...\n",
            "Loading republicans_quotes...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_ENCODINGS = True\n",
        "\n",
        "path_to_democrates_embeddings = os.path.join(\n",
        "    DATA_PATH, 'SBERT', f'embeddings_democrates_{YEAR}.pkl'\n",
        "    )\n",
        "path_to_republicans_embeddings = os.path.join(\n",
        "    DATA_PATH, 'SBERT', f'embeddings_republicans_{YEAR}.pkl'\n",
        "    )\n",
        "\n",
        "if not LOAD_ENCODINGS:\n",
        "    democrates_embeddings = model.encode(\n",
        "        democrates_tokens_per_quote_strings,\n",
        "        show_progress_bar=True,\n",
        "        device=DEVICE,\n",
        "        batch_size=32 if str(DEVICE) == 'cuda' else 1\n",
        "        )\n",
        "    \n",
        "    republicans_embeddings = model.encode(\n",
        "        republicans_tokens_per_quote_strings,\n",
        "        show_progress_bar=True,\n",
        "        device=DEVICE,\n",
        "        batch_size=32 if str(DEVICE) == 'cuda' else 1\n",
        "        )\n",
        "\n",
        "else:\n",
        "    print('Loading democrates_embeddings...')\n",
        "    democrates_embeddings = pickle.load(\n",
        "        open(path_to_democrates_embeddings, 'rb')\n",
        "        )\n",
        "    \n",
        "    print('Loading republicans_embeddings...')\n",
        "    republicans_embeddings = pickle.load(\n",
        "        open(path_to_republicans_embeddings, 'rb')\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8CbeHHgG_45",
        "outputId": "f4e47794-8e77-459e-b223-8cc02d75c3bd"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading democrates_embeddings...\n",
            "Loading republicans_embeddings...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_embeddings(topics, model):\n",
        "    topics_embeddings = model.encode(\n",
        "        topics, show_progress_bar=True, device=DEVICE, batch_size=1\n",
        "        )\n",
        "    \n",
        "    return topics_embeddings\n",
        "\n",
        "\n",
        "TOPICS = [\n",
        "    'economy', 'healthcare', 'education', 'russia', \n",
        "    'korea', 'trump', 'china', 'guns', 'budget'\n",
        "    ]\n",
        "\n",
        "topics_embeddings = get_topic_embeddings(TOPICS, model)\n",
        "\n",
        "print(topics_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "94b7470543774b6e946c3a2cc060be15",
            "1e08a06fc61f43bfbf06941d92a6dd91",
            "6b1435bb2ae5496ebc65356eb03eccc2",
            "fc9cc6db824045a681c541894eae87c3",
            "11f67c39683a40b89d0e786f8ffab5b4",
            "663ecfcba3844b13b3ee4d69d23f940d",
            "18d145cf20bd4189b30a0d1bf10885f2",
            "0ec9ad2941e3488c86dc88eb67f8c1be",
            "cdfc7651f41f431f9c414abdf4e6e751",
            "ac7a4b65cd564022aae38d29848b94d1",
            "e7e8ab938de54816928296ebca5991a6"
          ]
        },
        "id": "3kir1MhJMZA_",
        "outputId": "5dfbf20c-b344-4b38-c205-ed9525673974"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94b7470543774b6e946c3a2cc060be15",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "democrates_similarities_per_topic = {}\n",
        "republicans_similarities_per_topic = {}\n",
        "\n",
        "for topic, topic_embedding in zip(TOPICS, topics_embeddings):\n",
        "    print(topic)\n",
        "\n",
        "    democrates_similarities_per_topic[topic] = {}\n",
        "\n",
        "    democrates_similarities = compute_cosine_similarity(\n",
        "        torch.from_numpy(democrates_embeddings), \n",
        "        torch.from_numpy(topic_embedding)\n",
        "        )\n",
        "    \n",
        "    democrates_similarities_per_topic[topic]['sorted'], \\\n",
        "    democrates_similarities_per_topic[topic]['indexes'] = \\\n",
        "        torch.sort(democrates_similarities, descending=True)\n",
        "\n",
        "    \n",
        "\n",
        "    republicans_similarities_per_topic[topic] = {}\n",
        "\n",
        "    republicans_similarities = compute_cosine_similarity(\n",
        "        torch.from_numpy(republicans_embeddings), \n",
        "        torch.from_numpy(topic_embedding)\n",
        "        )\n",
        "    \n",
        "    republicans_similarities_per_topic[topic]['sorted'], \\\n",
        "    republicans_similarities_per_topic[topic]['indexes'] = \\\n",
        "        torch.sort(republicans_similarities, descending=True)\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT4jFFHIR1EK",
        "outputId": "1ff18523-acea-4713-96d6-73c01f683354"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "economy\n",
            "healthcare\n",
            "education\n",
            "russia\n",
            "korea\n",
            "trump\n",
            "china\n",
            "guns\n",
            "budget\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_quotes_for_topic(\n",
        "    all_quotes, \n",
        "    topic, \n",
        "    similarities_per_topic, \n",
        "    num_to_print=None\n",
        "    ):\n",
        "\n",
        "    num_quotes = len(similarities_per_topic[topic]['indexes'])\n",
        "    if num_to_print is None:\n",
        "        num_to_print = num_quotes\n",
        "    step = num_quotes / num_to_print\n",
        "\n",
        "    sampling = np.arange(0, num_quotes-1, step)\n",
        "\n",
        "    should_keep = list(np.array(\n",
        "        similarities_per_topic[topic]['indexes'][sampling],\n",
        "        ))\n",
        "\n",
        "    kept_similarities = similarities_per_topic[topic]['sorted'][sampling]\n",
        "    kept_quotes = itemgetter(*should_keep)(all_quotes)\n",
        "\n",
        "    for i in range(len(kept_quotes)):\n",
        "        print(f'{kept_similarities[i]} {kept_quotes[i]}')\n",
        "\n",
        "\n",
        "def get_similar_quotes(all_quotes, similarities_per_topic, topic, threshold):\n",
        "\n",
        "    should_keep = np.where(\n",
        "        similarities_per_topic[topic]['sorted'] > threshold\n",
        "        )[0]\n",
        "\n",
        "    kept_indexes = similarities_per_topic[topic]['indexes'][should_keep]\n",
        "\n",
        "    kept_quotes = itemgetter(*kept_indexes)(all_quotes)\n",
        "\n",
        "    return kept_quotes\n",
        "\n",
        "def get_number_of_similar_quotes_for_topic(\n",
        "    topic, \n",
        "    similarities_per_topic,\n",
        "    treshold\n",
        "    ):\n",
        "\n",
        "    procentage_quotes = np.mean(np.array(\n",
        "        similarities_per_topic[topic]['sorted'] > treshold, dtype=np.int8\n",
        "        ))\n",
        "\n",
        "    return procentage_quotes\n",
        "\n"
      ],
      "metadata": {
        "id": "ZAT2B-Gqo9Wv"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLDS_DICT = {'education': 0.37, 'trump': 0.32, 'china':32}\n",
        "\n",
        "democrates_quotes_per_topic = {}\n",
        "republicans_quotes_per_topic = {}\n",
        "\n",
        "for topic in ['trump']:\n",
        "    threshold = THRESHOLDS_DICT[topic]\n",
        "\n",
        "    democrates_quotes_per_topic[topic] = get_similar_quotes(\n",
        "        democrates_quotes, democrates_similarities_per_topic, topic, threshold\n",
        "        )\n",
        "    \n",
        "    republicans_quotes_per_topic[topic] = get_similar_quotes(\n",
        "        republicans_quotes, republicans_similarities_per_topic, topic, threshold\n",
        "        )\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "8HS5sD2YDw_6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ABSA"
      ],
      "metadata": {
        "id": "_5JSZxTeGFl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "SENTIMENT_INDEXING = {'neutral': 0, 'negative': 1, 'positive': 2}\n",
        "\n",
        "start = time()\n",
        "\n",
        "for i, quote in enumerate(democrates_quotes_per_topic['trump']):\n",
        "    print(i)\n",
        "\n",
        "    task = nlp(text=(quote), aspects=['trump'])\n",
        "    absa_scores = task.examples[0].scores\n",
        "\n",
        "    print('\\n')\n",
        "    print(quote)\n",
        "    for sentiment_str, ind in SENTIMENT_INDEXING.items():\n",
        "        print(f'{sentiment_str}: {absa_scores[ind]:.3f}')\n",
        "\n",
        "    if i == 50:\n",
        "        break\n",
        "\n",
        "end = time()\n",
        "\n",
        "print(end - start)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4Rd3snsKO7V",
        "outputId": "259c8ad7-93d0-4be6-d35c-e4a6192131bd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "\n",
            "\n",
            "Trump' s the man.\n",
            "neutral: 0.001\n",
            "negative: 0.001\n",
            "positive: 0.998\n",
            "1\n",
            "\n",
            "\n",
            "We need to pray for Trump,\n",
            "neutral: 0.333\n",
            "negative: 0.031\n",
            "positive: 0.636\n",
            "2\n",
            "\n",
            "\n",
            "This Trump is bad,\n",
            "neutral: 0.001\n",
            "negative: 0.991\n",
            "positive: 0.008\n",
            "3\n",
            "\n",
            "\n",
            "I don't want Trump to win. Did he win?\n",
            "neutral: 0.141\n",
            "negative: 0.849\n",
            "positive: 0.010\n",
            "4\n",
            "\n",
            "\n",
            "will help Trump win.\n",
            "neutral: 0.002\n",
            "negative: 0.001\n",
            "positive: 0.997\n",
            "5\n",
            "\n",
            "\n",
            "Trump Of The Day Club\n",
            "neutral: 0.006\n",
            "negative: 0.003\n",
            "positive: 0.991\n",
            "6\n",
            "\n",
            "\n",
            "Oh my gosh. Trump said that?\n",
            "neutral: 0.806\n",
            "negative: 0.008\n",
            "positive: 0.186\n",
            "7\n",
            "\n",
            "\n",
            "We're in a Trump era, but that's our goal.\n",
            "neutral: 0.011\n",
            "negative: 0.003\n",
            "positive: 0.986\n",
            "8\n",
            "\n",
            "\n",
            "What has he [ Trump ] done to slow up Iran at all?\n",
            "neutral: 0.629\n",
            "negative: 0.349\n",
            "positive: 0.022\n",
            "9\n",
            "\n",
            "\n",
            "[ Trump ] shouldn't do it at all,\n",
            "neutral: 0.157\n",
            "negative: 0.618\n",
            "positive: 0.225\n",
            "10\n",
            "\n",
            "\n",
            "Well, you should vote for Trump,\n",
            "neutral: 0.026\n",
            "negative: 0.002\n",
            "positive: 0.972\n",
            "11\n",
            "\n",
            "\n",
            "What a gift Trump gave Beto,\n",
            "neutral: 0.001\n",
            "negative: 0.001\n",
            "positive: 0.998\n",
            "12\n",
            "\n",
            "\n",
            "the day after Trump,\n",
            "neutral: 0.980\n",
            "negative: 0.004\n",
            "positive: 0.016\n",
            "13\n",
            "\n",
            "\n",
            "This is not trumped up,\n",
            "neutral: 0.886\n",
            "negative: 0.057\n",
            "positive: 0.057\n",
            "14\n",
            "\n",
            "\n",
            "But they were good men and Trump isn't.\n",
            "neutral: 0.300\n",
            "negative: 0.408\n",
            "positive: 0.292\n",
            "15\n",
            "\n",
            "\n",
            "the rule of Trump.\n",
            "neutral: 0.007\n",
            "negative: 0.002\n",
            "positive: 0.992\n",
            "16\n",
            "\n",
            "\n",
            "only Trump uses that word.\n",
            "neutral: 0.743\n",
            "negative: 0.223\n",
            "positive: 0.033\n",
            "17\n",
            "\n",
            "\n",
            "a lot like Trump.\n",
            "neutral: 0.024\n",
            "negative: 0.005\n",
            "positive: 0.972\n",
            "18\n",
            "\n",
            "\n",
            "has to be that we have to beat Trump.\n",
            "neutral: 0.231\n",
            "negative: 0.207\n",
            "positive: 0.562\n",
            "19\n",
            "\n",
            "\n",
            "It's over, Trump,\n",
            "neutral: 0.007\n",
            "negative: 0.003\n",
            "positive: 0.990\n",
            "20\n",
            "\n",
            "\n",
            "Trump got his Roy Cohn,\n",
            "neutral: 0.015\n",
            "negative: 0.003\n",
            "positive: 0.982\n",
            "21\n",
            "\n",
            "\n",
            "He knew because he knew [ Mr. Trump ] more than any of us did.\n",
            "neutral: 0.771\n",
            "negative: 0.025\n",
            "positive: 0.204\n",
            "22\n",
            "\n",
            "\n",
            "Well, Rush, but why does Trump have to lie?\n",
            "neutral: 0.053\n",
            "negative: 0.929\n",
            "positive: 0.018\n",
            "23\n",
            "\n",
            "\n",
            "You should vote for Trump.\n",
            "neutral: 0.041\n",
            "negative: 0.003\n",
            "positive: 0.955\n",
            "24\n",
            "\n",
            "\n",
            "this is where they get Trump\n",
            "neutral: 0.003\n",
            "negative: 0.001\n",
            "positive: 0.996\n",
            "25\n",
            "\n",
            "\n",
            "So, Trump took a loss,\n",
            "neutral: 0.037\n",
            "negative: 0.842\n",
            "positive: 0.121\n",
            "26\n",
            "\n",
            "\n",
            "not out of love for Trump,\n",
            "neutral: 0.221\n",
            "negative: 0.631\n",
            "positive: 0.149\n",
            "27\n",
            "\n",
            "\n",
            "With Trump it is not easy,\n",
            "neutral: 0.052\n",
            "negative: 0.819\n",
            "positive: 0.129\n",
            "28\n",
            "\n",
            "\n",
            "well, that's the end of Trump,\n",
            "neutral: 0.224\n",
            "negative: 0.152\n",
            "positive: 0.624\n",
            "29\n",
            "\n",
            "\n",
            "That's what Trump doesn't want us to do.\n",
            "neutral: 0.227\n",
            "negative: 0.485\n",
            "positive: 0.288\n",
            "30\n",
            "\n",
            "\n",
            "Yes. You'd find it at the Trump Org.\n",
            "neutral: 0.107\n",
            "negative: 0.014\n",
            "positive: 0.879\n",
            "31\n",
            "\n",
            "\n",
            "We don't! But Trump does.\n",
            "neutral: 0.007\n",
            "negative: 0.002\n",
            "positive: 0.991\n",
            "32\n",
            "\n",
            "\n",
            "Yeah, we've got to deal with Trump,\n",
            "neutral: 0.017\n",
            "negative: 0.002\n",
            "positive: 0.981\n",
            "33\n",
            "\n",
            "\n",
            "has a bit of the Trump gene,\n",
            "neutral: 0.001\n",
            "negative: 0.001\n",
            "positive: 0.997\n",
            "34\n",
            "\n",
            "\n",
            "That's how we beat Trump,\n",
            "neutral: 0.005\n",
            "negative: 0.003\n",
            "positive: 0.992\n",
            "35\n",
            "\n",
            "\n",
            "In the life of Trump,\n",
            "neutral: 0.133\n",
            "negative: 0.007\n",
            "positive: 0.860\n",
            "36\n",
            "\n",
            "\n",
            "Trump: `Why don't they go back? '\n",
            "neutral: 0.795\n",
            "negative: 0.180\n",
            "positive: 0.025\n",
            "37\n",
            "\n",
            "\n",
            "take back all that Trump took away.\n",
            "neutral: 0.049\n",
            "negative: 0.808\n",
            "positive: 0.143\n",
            "38\n",
            "\n",
            "\n",
            "Who is a fan of Mr Trump?\n",
            "neutral: 0.531\n",
            "negative: 0.018\n",
            "positive: 0.451\n",
            "39\n",
            "\n",
            "\n",
            "I don't know why Trump has such hate for Cuba,\n",
            "neutral: 0.091\n",
            "negative: 0.629\n",
            "positive: 0.280\n",
            "40\n",
            "\n",
            "\n",
            "beat [ Trump ] again,\n",
            "neutral: 0.494\n",
            "negative: 0.078\n",
            "positive: 0.427\n",
            "41\n",
            "\n",
            "\n",
            "No, we don't, but Trump does,\n",
            "neutral: 0.005\n",
            "negative: 0.002\n",
            "positive: 0.993\n",
            "42\n",
            "\n",
            "\n",
            "I'm the guy who can beat Trump\n",
            "neutral: 0.080\n",
            "negative: 0.012\n",
            "positive: 0.909\n",
            "43\n",
            "\n",
            "\n",
            "Trump's tax scam.\n",
            "neutral: 0.024\n",
            "negative: 0.911\n",
            "positive: 0.066\n",
            "44\n",
            "\n",
            "\n",
            "Trump is the one who is doing that,\n",
            "neutral: 0.010\n",
            "negative: 0.002\n",
            "positive: 0.988\n",
            "45\n",
            "\n",
            "\n",
            "I'm not sure if it will be very much. If you have Trump on for the sake of having it, it's not so good,\n",
            "neutral: 0.043\n",
            "negative: 0.838\n",
            "positive: 0.119\n",
            "46\n",
            "\n",
            "\n",
            "Hey, SDNY, it's not their job to get rid of Trump.\n",
            "neutral: 0.548\n",
            "negative: 0.390\n",
            "positive: 0.062\n",
            "47\n",
            "\n",
            "\n",
            "get rid of him (Trump)\n",
            "neutral: 0.334\n",
            "negative: 0.071\n",
            "positive: 0.596\n",
            "48\n",
            "\n",
            "\n",
            "Tell Trump not to come here.\n",
            "neutral: 0.042\n",
            "negative: 0.935\n",
            "positive: 0.023\n",
            "49\n",
            "\n",
            "\n",
            "Trump Wins We Lose.\n",
            "neutral: 0.048\n",
            "negative: 0.023\n",
            "positive: 0.929\n",
            "50\n",
            "\n",
            "\n",
            "will beat Trump's ass.\n",
            "neutral: 0.007\n",
            "negative: 0.012\n",
            "positive: 0.981\n",
            "60.03360843658447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SENTIMENT_INDEXING = {'neutral': 0, 'negative': 1, 'positive': 2}\n",
        "\n",
        "for topic in ['trump']:\n",
        "    \n",
        "    task = nlp(text=(democrates_quotes_per_topic[topic]), aspects=[topic])\n",
        "    absa_scores = task.examples[0].scores\n",
        "\n",
        "    for sentiment_str, ind in SENTIMENT_INDEXING.items():\n",
        "        print(f'{sentiment_str}: {absa_scores[ind]:.3f}')\n",
        "\n",
        "    \n",
        "    task = nlp(text=(republicans_quotes_per_topic[topic]), aspects=[topic])\n",
        "    absa_scores = task.examples[0].scores\n",
        "\n",
        "    for sentiment_str, ind in SENTIMENT_INDEXING.items():\n",
        "        print(f'{sentiment_str}: {absa_scores[ind]:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "-s6bS08I7pzt",
        "outputId": "16d6347c-91f7-49bf-fb74-e87456978544"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-9224bb63e7e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'trump'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemocrates_quotes_per_topic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mabsa_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/aspect_based_sentiment_analysis/pipelines.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, aspects)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCompletedTask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0mcompleted_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompleted_task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/aspect_based_sentiment_analysis/pipelines.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPredictedExample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtokenized_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0minput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0moutput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/aspect_based_sentiment_analysis/pipelines.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTokenizedExample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0malignment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTokenizedExample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mInputBatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/aspect_based_sentiment_analysis/pipelines.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTokenizedExample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0malignment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTokenizedExample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mInputBatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/aspect_based_sentiment_analysis/alignment.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(tokenizer, text, aspect)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mwordpiece_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;31m# union() returns a new set by concatenating the two sets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mnever_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnever_split\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xFFFD\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ord() expected a character, but string of length 17 found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oCYAjYYXGbD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print "
      ],
      "metadata": {
        "id": "De6rsKduEgpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_quotes_for_topic(\n",
        "    republicans_quotes, 'education', republicans_similarities_per_topic, 1000\n",
        "    )\n"
      ],
      "metadata": {
        "id": "OGRtd1eMfmB9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_quotes_for_topic(\n",
        "    republicans_quotes, 'china', republicans_similarities_per_topic, 1000\n",
        "    )"
      ],
      "metadata": {
        "id": "-bmXPwKn9_Om"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "democrates_quotes_per_topic['trump']"
      ],
      "metadata": {
        "id": "z0iKjec44i8H"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eHoCCMaB4tCL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}