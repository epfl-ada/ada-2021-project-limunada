{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj1adpuQqB_k"
   },
   "source": [
    "# BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cP0jcOMp6LB"
   },
   "source": [
    "## Instalations and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1shpoO0al7a4"
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jmIQW9BPFgzk",
    "outputId": "b27be721-fa34-45f4-bfbb-4e0611d5b2e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "PROJECT_ROOT = '/content/drive/MyDrive'\n",
    "\n",
    "import os \n",
    "os.chdir(PROJECT_ROOT)\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, 'Quotebank_limunADA')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import scipy\n",
    "import pickle\n",
    "import bz2\n",
    "import json\n",
    "from operator import itemgetter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2FDZq8WVjnE"
   },
   "source": [
    "## Text prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cIyZpZNnFuKE"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "\n",
    "    return lda_tokens\n",
    "\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YB3yOi4mpxRC"
   },
   "source": [
    "## Generating tokens and embeddings for quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hb-raSxhV9pw"
   },
   "outputs": [],
   "source": [
    "# Loading tokens from a file\n",
    "def get_tokens_per_quote(path_to_file, print_step=5e4, num_instances=None):\n",
    "\n",
    "    tokens_per_quote = {}\n",
    "    # Iterate through the quotes\n",
    "    with bz2.open(path_to_file, 'rb') as s_file:\n",
    "        for i, instance in enumerate(s_file):\n",
    "            if i % int(print_step) == 0:\n",
    "                print(f'Instance {i}')\n",
    "\n",
    "            if num_instances is not None:\n",
    "                if i == num_instances:\n",
    "                    break \n",
    "\n",
    "            # loading a sample and checking the speaker\n",
    "            instance = json.loads(instance) \n",
    "            tokens = prepare_text_for_lda(instance['quotation'])\n",
    "\n",
    "            tokens_per_quote[instance['quoteID']] = tokens \n",
    "    \n",
    "    return tokens_per_quote\n",
    "\n",
    "# Loading quotes at given indexes from a file\n",
    "def get_instances_at_indexes(path_to_file, indexes=None, print_step=5e4):\n",
    "    instances = []\n",
    "    # Iterate through the quotes\n",
    "    with bz2.open(path_to_file, 'rb') as s_file:\n",
    "        for i, instance in enumerate(s_file):\n",
    "            if i % int(print_step) == 0:\n",
    "                print(f'Instance {i}')\n",
    "                \n",
    "            if indexes is None or i in indexes:\n",
    "                instance = json.loads(instance) \n",
    "                instances.append(instance['quotation'])\n",
    "            else:                \n",
    "                continue \n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Fd7S3aXSX1XB"
   },
   "outputs": [],
   "source": [
    "def load_tokens_string_per_quote(party, year, load_tokens_per_quote=True):\n",
    "  \"\"\"\n",
    "  Given a party and a year loads all the tokens per quote such that each \n",
    "  quote is assigned to a string that is concatenation of tokens and spaces. \n",
    "  Additionally, if load_tokens_per_quote is false, than generates all the tokens\n",
    "  \"\"\"\n",
    "\n",
    "  # Get path to json file to cleaned data from Quotebank\n",
    "  path_to_file = os.path.join(  \n",
    "      DATA_PATH, f'quotes-{party}-{year}.json.bz2'\n",
    "      )\n",
    "\n",
    "  # Make new directory if necessary\n",
    "  os.makedirs(os.path.join(DATA_PATH, 'SBERT'), exist_ok=True) \n",
    "\n",
    "  # Get path to file with tokens\n",
    "  path_to_tokens = os.path.join(\n",
    "      DATA_PATH, 'SBERT', f'tokens_per_quote_strings_{party}_{year}.pkl'\n",
    "      )\n",
    "\n",
    "  # Check if we should only load or generate tokens\n",
    "  if not load_tokens_per_quote:\n",
    "    # Get tokens for each quote\n",
    "    tokens_per_quote = get_tokens_per_quote(\n",
    "        path_to_file, num_instances=None\n",
    "        )\n",
    "   \n",
    "    # For each quote generate a string from all the tokens by concatenation\n",
    "    tokens_per_quote_strings = \\\n",
    "        [' '.join(quote) for k, quote in tokens_per_quote.items()]\n",
    "  \n",
    "    # Save all the tokens\n",
    "    pickle.dump(\n",
    "        tokens_per_quote_strings, \n",
    "        open(path_to_tokens, 'wb')\n",
    "        )\n",
    "    \n",
    "  else:\n",
    "    # Load tokens\n",
    "    print(f'Loading {party}_tokens_per_quote_strings...')\n",
    "    tokens_per_quote_strings = pickle.load(\n",
    "        open(path_to_tokens, 'rb')          \n",
    "        )\n",
    "  \n",
    "  return tokens_per_quote_strings\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "31xijkyEcBpl"
   },
   "outputs": [],
   "source": [
    "def load_quotes(party, year, load_quotes=True):\n",
    "  \"\"\"\n",
    "  Given a party and a year, loads all the quotes as a list of strings\n",
    "  \"\"\"\n",
    "  # Get a path to json file to cleaned data from Quotebank\n",
    "  path_to_file = os.path.join(  \n",
    "      DATA_PATH, f'quotes-{party}-{year}.json.bz2'\n",
    "      )\n",
    "\n",
    "  # Get a path to output file where quotes will be stored so the access to them\n",
    "  # is more efficient\n",
    "  path_to_quotes_list = os.path.join(\n",
    "      DATA_PATH, 'SBERT', f'quotes_list_{party}_{year}.pkl'\n",
    "      )\n",
    "\n",
    "  if not load_quotes:\n",
    "    # Load all the quotes from initial file\n",
    "    quotes = get_instances_at_indexes(path_to_file)\n",
    "      \n",
    "    # Store all the quotes \n",
    "    pickle.dump(\n",
    "        quotes, \n",
    "        open(path_to_quotes_list, 'wb')\n",
    "        ) \n",
    "    \n",
    "  else:\n",
    "    print(f'Loading {party}_quotes...')\n",
    "    quotes = pickle.load(\n",
    "        open(path_to_quotes_list, 'rb')\n",
    "        )\n",
    "  \n",
    "  return quotes\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "h2UAIDSxerVF"
   },
   "outputs": [],
   "source": [
    "def load_embeddings(party, year, load_embeddings=True):\n",
    "  \"\"\"\n",
    "  Given a party and a year returns a list of embeddings for each quote\n",
    "  Additionally, if load embeddings is false than it generates all the embeddings\n",
    "  and stores it.\n",
    "  \"\"\"\n",
    "\n",
    "  # Get path to file with the embeddings\n",
    "  path_to_embeddings = os.path.join(\n",
    "    DATA_PATH, 'SBERT', f'embeddings_{party}_{year}.pkl'\n",
    "    )\n",
    "\n",
    "  # Check if embeddings should be generated\n",
    "  if not load_embeddings:\n",
    "    # Loads all the tokens \n",
    "    tokens_per_quote_strings = load_tokens_string_per_quote(party, year, load_tokens_per_quote=True)\n",
    "\n",
    "    # Generates all the embeddings by using SentenceTransfromer model    \n",
    "    embeddings = model.encode(\n",
    "        tokens_per_quote_strings,\n",
    "        show_progress_bar=True,\n",
    "        device=DEVICE,\n",
    "        batch_size=32 if str(DEVICE) == 'cuda' else 1\n",
    "        )\n",
    "    \n",
    "    # Store all the embeddings \n",
    "    pickle.dump(\n",
    "        embeddings, \n",
    "        open(path_to_embeddings, 'wb')\n",
    "        ) \n",
    "\n",
    "  else:\n",
    "    print(f'Loading {party}_embeddings...')\n",
    "    # Loads the embeddings\n",
    "    embeddings = pickle.load(\n",
    "        open(path_to_embeddings, 'rb')\n",
    "        )\n",
    "    \n",
    "  # Returns embeddings\n",
    "  return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "LXlyq6l8pPns",
    "outputId": "f81a439e-6aa7-4b6e-e4d9-a2591b261591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-455015f05641>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2015\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2016\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2017\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mparty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'republicans'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'democrates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtokens_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tokens_string_per_quote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_tokens_per_quote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mquotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_quotes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_quotes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3b7411eec873>\u001b[0m in \u001b[0;36mload_tokens_string_per_quote\u001b[0;34m(party, year, load_tokens_per_quote)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Get tokens for each quote\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     tokens_per_quote = get_tokens_per_quote(\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_instances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         )\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-78ff047b5eda>\u001b[0m in \u001b[0;36mget_tokens_per_quote\u001b[0;34m(path_to_file, print_step, num_instances)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# loading a sample and checking the speaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_text_for_lda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quotation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtokens_per_quote\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quoteID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d26b22d644e9>\u001b[0m in \u001b[0;36mprepare_text_for_lda\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0men_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_lemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d26b22d644e9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0men_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_lemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d26b22d644e9>\u001b[0m in \u001b[0;36mget_lemma\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_lemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__reader_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# This is where the magic happens!  Transform ourselves into\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;31m# Load the indices for lemmas and synset offsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_lemma_pos_offset_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# load the exception file data into memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_load_lemma_pos_offset_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;31m# parse each line of the file (ignoring comment lines)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index.%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0;34m\"\"\"Return the next decoded line from the underlying stream.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mnew_chars\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m             \u001b[0mchars\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### Novakovic\n",
    "for year in [2015, 2016, 2017]:\n",
    "  for party in ['republicans', 'democrates']:\n",
    "    tokens_string = load_tokens_string_per_quote(party, year, load_tokens_per_quote=False)\n",
    "    quotes = load_quotes(party, year, load_quotes=False)\n",
    "    embeddings = load_embeddings(party, year, load_embeddings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhGBdGeVrvVs"
   },
   "source": [
    "## Generating embeddings for topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "9d58e77f2bd242efbc20409487e70318",
      "3ff08a2aca494d6086b5b10c3d54f331",
      "fc2569b6cbe74a4a8ec6d444e11ba29c",
      "294ff4e30ef94d26839a627f1c1e8e35",
      "5e4efea9d4044aa1a3d1a1c3dedcb8d2",
      "3ee021db317a4463b43e4e359014aaf8",
      "735ff65ccf4d40c2b7bf1b3c6a771f69",
      "39599ad0ff8144108610489da680672f",
      "c8fa123601904df185e15407f8a49008",
      "943e2fa970e049cda0aa9ec69c38559c",
      "ff85e203a4294a709a6e5022530b1945"
     ]
    },
    "id": "3kir1MhJMZA_",
    "outputId": "7334c1e4-13e9-4bae-cfa4-f034da66b919"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d58e77f2bd242efbc20409487e70318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 384)\n"
     ]
    }
   ],
   "source": [
    "def get_topic_embeddings(topics, model):\n",
    "  \"\"\"\n",
    "  Generates topic embeddings from pretrained model\n",
    "  \"\"\"\n",
    "  topics_embeddings = model.encode(\n",
    "      topics, show_progress_bar=True, device=DEVICE, batch_size=1\n",
    "      )\n",
    "    \n",
    "  return topics_embeddings\n",
    "\n",
    "\n",
    "TOPICS = [\n",
    "    'politics', 'economy', 'education', 'health', 'crime'\n",
    "    ,'russia', 'korea', 'trump', 'china', \n",
    "    'drug addiction', 'climate change', 'racism', 'terrorism',\n",
    "    'illegal immigration', 'sexism', 'affordability of healthcare',\n",
    "    'affordability of college education', 'economic inequality', \n",
    "    'job opportunities'\n",
    "    ]\n",
    "\n",
    "topics_embeddings = get_topic_embeddings(TOPICS, model)\n",
    "\n",
    "print(topics_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apmh4FKsy0VT",
    "outputId": "2895825e-7355-465c-d563-a28592e56e38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading democrates_embeddings...\n",
      "politics\n",
      "economy\n",
      "education\n",
      "health\n",
      "crime\n",
      "russia\n",
      "korea\n",
      "trump\n",
      "china\n",
      "drug addiction\n",
      "climate change\n",
      "racism\n",
      "terrorism\n",
      "illegal immigration\n",
      "sexism\n",
      "affordability of healthcare\n",
      "affordability of college education\n",
      "economic inequality\n",
      "job opportunities\n",
      "Loading democrates_quotes...\n",
      "Loading republicans_embeddings...\n",
      "politics\n",
      "economy\n",
      "education\n",
      "health\n",
      "crime\n",
      "russia\n",
      "korea\n",
      "trump\n",
      "china\n",
      "drug addiction\n",
      "climate change\n",
      "racism\n",
      "terrorism\n",
      "illegal immigration\n",
      "sexism\n",
      "affordability of healthcare\n",
      "affordability of college education\n",
      "economic inequality\n",
      "job opportunities\n",
      "Loading republicans_quotes...\n",
      "DEMOCRATES\n",
      "Length 19384\n",
      "Ratio 0.018330595659249885\n",
      "Procentage of postive 5.581923235658275%\n",
      "Procentage of neutral 91.0699546017334%\n",
      "Procentage of negative 3.348122162608337%\n",
      "\n",
      "REPUBLICANS\n",
      "Length 16952\n",
      "Ratio 0.020603170204646165\n",
      "Procentage of postive 6.040585181689476%\n",
      "Procentage of neutral 90.44360547428032%\n",
      "Procentage of negative 3.5158093440302034%\n"
     ]
    }
   ],
   "source": [
    "def get_similarities_per_topis(party, year):\n",
    "  compute_cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "  similarities_per_topic = {}\n",
    "\n",
    "  embeddings = load_embeddings(party, year)\n",
    "\n",
    "  for topic, topic_embedding in zip(TOPICS, topics_embeddings):\n",
    "    print(topic)\n",
    "\n",
    "    similarities_per_topic[topic] = {}\n",
    "\n",
    "    similarities = compute_cosine_similarity(\n",
    "        torch.from_numpy(embeddings), \n",
    "        torch.from_numpy(topic_embedding)\n",
    "        )\n",
    "    \n",
    "    similarities_per_topic[topic]['sorted'], \\\n",
    "    similarities_per_topic[topic]['indexes'] = \\\n",
    "        torch.sort(similarities, descending=True)\n",
    "\n",
    "  return similarities_per_topic\n",
    "\n",
    "def get_similar_quotes(all_quotes, similarities_per_topic, topic, threshold):\n",
    "\n",
    "    should_keep = np.where(\n",
    "        similarities_per_topic[topic]['sorted'] > threshold\n",
    "        )[0]\n",
    "\n",
    "    kept_indexes = similarities_per_topic[topic]['indexes'][should_keep]\n",
    "\n",
    "    kept_quotes = itemgetter(*kept_indexes)(all_quotes)\n",
    "\n",
    "    return kept_quotes\n",
    "\n",
    "def get_analysis_of_similar_quotes(all_quotes, similarities_per_topic, topic, threshold):\n",
    "  similar_quotes = get_similar_quotes(all_quotes, similarities_per_topic, topic, threshold)\n",
    "  print(f'Length {len(similar_quotes)}')\n",
    "  print(f'Ratio {len(similar_quotes) / len(all_quotes)}')\n",
    "  get_sentiment = lambda x: 1 if x > 0.3 else (0 if x > -0.3 else -1)\n",
    "  sentiments = np.array([get_sentiment(TextBlob(quote).sentiment.polarity) for quote in similar_quotes])\n",
    "  print(f'Procentage of postive {np.mean(sentiments == 1)*100}%')\n",
    "  print(f'Procentage of neutral {np.mean(sentiments == 0)*100}%')\n",
    "  print(f'Procentage of negative {np.mean(sentiments == -1)*100}%')\n",
    "  return\n",
    "\n",
    "democrates_2019_similarities_per_topic = get_similarities_per_topis('democrates', 2019)\n",
    "democrates_2019_quotes = load_quotes('democrates', 2019)\n",
    "republicans_2019_similarities_per_topic = get_similarities_per_topis('republicans', 2019)\n",
    "republicans_2019_quotes = load_quotes('republicans', 2019)\n",
    "treshold = 0.4\n",
    "print('DEMOCRATES')\n",
    "get_analysis_of_similar_quotes(democrates_2019_quotes, democrates_2019_similarities_per_topic, 'trump', treshold)\n",
    "print()\n",
    "print('REPUBLICANS')\n",
    "get_analysis_of_similar_quotes(republicans_2019_quotes, republicans_2019_similarities_per_topic, 'trump', treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZAT2B-Gqo9Wv"
   },
   "outputs": [],
   "source": [
    "def print_quotes_for_topic(\n",
    "    all_quotes, \n",
    "    topic, \n",
    "    similarities_per_topic, \n",
    "    num_to_print=None\n",
    "    ):\n",
    "\n",
    "    num_quotes = len(similarities_per_topic[topic]['indexes'])\n",
    "    if num_to_print is None:\n",
    "        num_to_print = num_quotes\n",
    "    step = num_quotes / num_to_print\n",
    "\n",
    "    sampling = np.arange(0, num_quotes-1, step)\n",
    "\n",
    "    should_keep = list(np.array(\n",
    "        similarities_per_topic[topic]['indexes'][sampling],\n",
    "        ))\n",
    "\n",
    "    kept_similarities = similarities_per_topic[topic]['sorted'][sampling]\n",
    "    kept_quotes = itemgetter(*should_keep)(all_quotes)\n",
    "\n",
    "    for i in range(len(kept_quotes)):\n",
    "        print(f'{kept_similarities[i]} {kept_quotes[i]}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qP7NSPNt0tb9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "norDIodtrg-y"
   },
   "source": [
    "## Education\n",
    "\n",
    "We printed totally 1000 quotations. Rough estimation for a good treshold would be 0.37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d4ASiJrqFy4"
   },
   "outputs": [],
   "source": [
    "print_quotes_for_topic(\n",
    "    democrates_quotes, 'education', democrates_similarities_per_topic, 1000\n",
    "    )\n",
    "\n",
    "# TOPICS = [\n",
    "#     'economy', 'healthcare', 'education': 0.37, 'russia', \n",
    "#     'korea', 'trump', 'china', 'guns', 'budget'\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGRtd1eMfmB9"
   },
   "outputs": [],
   "source": [
    "print_quotes_for_topic(\n",
    "    republicans_quotes, 'education', republicans_similarities_per_topic, 1000\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISjk1Wwvw-E1"
   },
   "outputs": [],
   "source": [
    "democrates_quotes_education = get_similar_quotes(\n",
    "    democrates_quotes, democrates_similarities_per_topic, 'education', 0.37\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjLxfg7SxEVx"
   },
   "outputs": [],
   "source": [
    "republicans_quotes_education = get_similar_quotes(\n",
    "    republicans_quotes, republicans_similarities_per_topic, 'education', 0.37\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXsZdH99x02f"
   },
   "outputs": [],
   "source": [
    "THRESHOLDS_DICT = {'education': 0.37, 'trump': None}\n",
    "\n",
    "democrates_quotes_per_topic = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGmknvgFx0zl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "294ff4e30ef94d26839a627f1c1e8e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8fa123601904df185e15407f8a49008",
      "max": 19,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_39599ad0ff8144108610489da680672f",
      "value": 19
     }
    },
    "39599ad0ff8144108610489da680672f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ee021db317a4463b43e4e359014aaf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ff08a2aca494d6086b5b10c3d54f331": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e4efea9d4044aa1a3d1a1c3dedcb8d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff85e203a4294a709a6e5022530b1945",
      "placeholder": "​",
      "style": "IPY_MODEL_943e2fa970e049cda0aa9ec69c38559c",
      "value": " 19/19 [00:00&lt;00:00, 67.69it/s]"
     }
    },
    "735ff65ccf4d40c2b7bf1b3c6a771f69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "943e2fa970e049cda0aa9ec69c38559c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d58e77f2bd242efbc20409487e70318": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc2569b6cbe74a4a8ec6d444e11ba29c",
       "IPY_MODEL_294ff4e30ef94d26839a627f1c1e8e35",
       "IPY_MODEL_5e4efea9d4044aa1a3d1a1c3dedcb8d2"
      ],
      "layout": "IPY_MODEL_3ff08a2aca494d6086b5b10c3d54f331"
     }
    },
    "c8fa123601904df185e15407f8a49008": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc2569b6cbe74a4a8ec6d444e11ba29c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_735ff65ccf4d40c2b7bf1b3c6a771f69",
      "placeholder": "​",
      "style": "IPY_MODEL_3ee021db317a4463b43e4e359014aaf8",
      "value": "Batches: 100%"
     }
    },
    "ff85e203a4294a709a6e5022530b1945": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
